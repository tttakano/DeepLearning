{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "\n",
    "from nltk.translate import bleu_score\n",
    "import numpy\n",
    "#import progressbar\n",
    "import six\n",
    "import sys\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "UNK = 0\n",
    "EOS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_embed(embed, xs):\n",
    "    x_len = [len(x) for x in xs]\n",
    "    x_section = numpy.cumsum(x_len[:-1])\n",
    "    eee=F.concat(xs,axis=0)\n",
    "    ex = embed(F.concat(xs, axis=0))\n",
    "    exs = F.split_axis(ex, x_section, 0)\n",
    "    return exs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_layers, n_source_vocab, n_target_vocab, n_units):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed_x = L.EmbedID(n_source_vocab, n_units)\n",
    "            self.embed_y = L.EmbedID(n_target_vocab, n_units)\n",
    "            self.encoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            self.decoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            self.W = L.Linear(n_units, n_target_vocab)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "\n",
    "    def __call__(self, xs, ys):\n",
    "        xs = [x[::-1] for x in xs]                                  #reverse input      [\"i\", \"am\", \"taro\"] â†’[\"taro\", \"am\", \"I\"]\n",
    "\n",
    "        eos = self.xp.array([EOS], numpy.int32)\n",
    "        ys_in = [F.concat([eos, y], axis=0) for y in ys]      #[eos,y1,y2,...]\n",
    "        ys_out = [F.concat([y, eos], axis=0) for y in ys]     #[y1,y2,...,eos]\n",
    "\n",
    "        # Both xs and ys_in are lists of arrays.\n",
    "        exs = sequence_embed(self.embed_x, xs)\n",
    "        eys = sequence_embed(self.embed_y, ys_in)\n",
    "\n",
    "        batch = len(xs)\n",
    "        # None represents a zero vector in an encoder.\n",
    "        hx, cx, _ = self.encoder(None, None, exs)        #hidden_state, c_param, output =  encoder(hiddenn_state, preb_c, words)\n",
    "        _, _, os = self.decoder(hx, cx, eys)                      #####todo?         os:output for each sentence\n",
    "\n",
    "        # It is faster to concatenate data before calculating loss\n",
    "        # because only one matrix multiplication is called.\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_ys_out = F.concat(ys_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_ys_out, reduce='no')) / batch         #self.W: vector to id       calucurate loss\n",
    "\n",
    "        chainer.report({'loss': loss.data}, self)\n",
    "        n_words = concat_ys_out.shape[0]                           #output + eos length\n",
    "        perp = self.xp.exp(loss.data * batch / n_words)\n",
    "        chainer.report({'perp': perp}, self)\n",
    "        return loss\n",
    "\n",
    "    def translate(self, xs, max_length=100):\n",
    "        batch = len(xs)\n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            xs = [x[::-1] for x in xs]\n",
    "            exs = sequence_embed(self.embed_x, xs)\n",
    "            h, c, _ = self.encoder(None, None, exs)\n",
    "            ys = self.xp.full(batch, EOS, numpy.int32)         #create new array with full EOS\n",
    "            result = []\n",
    "            for i in range(max_length):\n",
    "                eys = self.embed_y(ys)\n",
    "                eys = F.split_axis(eys, batch, 0)                     \n",
    "                h, c, ys = self.decoder(h, c, eys)\n",
    "                cys = F.concat(ys, axis=0)\n",
    "                wy = self.W(cys)                                                #vector to id\n",
    "                ys = self.xp.argmax(wy.data, axis=1).astype(numpy.int32)         #select word\n",
    "                result.append(ys)\n",
    "\n",
    "        # Using `xp.concatenate(...)` instead of `xp.stack(result)` here to\n",
    "        # support NumPy 1.9.\n",
    "        result = cuda.to_cpu(\n",
    "            self.xp.concatenate([self.xp.expand_dims(x, 0) for x in result]).T)\n",
    "\n",
    "        # Remove EOS taggs\n",
    "        outs = []\n",
    "        for y in result:\n",
    "            inds = numpy.argwhere(y == EOS)\n",
    "            if len(inds) > 0:\n",
    "                y = y[:inds[0, 0]]\n",
    "            outs.append(y)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(batch, device):\n",
    "    def to_device_batch(batch):\n",
    "        if device is None:\n",
    "            return batch\n",
    "        elif device < 0:\n",
    "            return [chainer.dataset.to_device(device, x) for x in batch]\n",
    "        else:\n",
    "            xp = cuda.cupy.get_array_module(*batch)\n",
    "            concat = xp.concatenate(batch, axis=0)\n",
    "            sections = numpy.cumsum([len(x)\n",
    "                                     for x in batch[:-1]], dtype=numpy.int32)\n",
    "            concat_dev = chainer.dataset.to_device(device, concat)\n",
    "            batch_dev = cuda.cupy.split(concat_dev, sections)\n",
    "            return batch_dev\n",
    "\n",
    "    return {'xs': to_device_batch([x for x, _ in batch]),\n",
    "            'ys': to_device_batch([y for _, y in batch])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CalculateBleu(chainer.training.Extension):\n",
    "\n",
    "    trigger = 1, 'epoch'\n",
    "    priority = chainer.training.PRIORITY_WRITER\n",
    "\n",
    "    def __init__(self, model, test_data, key, batch=100, device=-1, max_length=100):\n",
    "        self.model = model\n",
    "        self.test_data = test_data\n",
    "        self.key = key\n",
    "        self.batch = batch\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, trainer):\n",
    "        with chainer.no_backprop_mode():\n",
    "            references = []\n",
    "            hypotheses = []\n",
    "            for i in range(0, len(self.test_data), self.batch):\n",
    "                sources, targets = zip(*self.test_data[i:i + self.batch])\n",
    "                references.extend([[t.tolist()] for t in targets])\n",
    "\n",
    "                sources = [\n",
    "                    chainer.dataset.to_device(self.device, x) for x in sources]\n",
    "                ys = [y.tolist()\n",
    "                      for y in self.model.translate(sources, self.max_length)]\n",
    "                hypotheses.extend(ys)\n",
    "\n",
    "        bleu = bleu_score.corpus_bleu(\n",
    "            references, hypotheses,\n",
    "            smoothing_function=bleu_score.SmoothingFunction().method1)\n",
    "        chainer.report({self.key: bleu})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_lines(path):\n",
    "    with open(path) as f:\n",
    "        return sum([1 for _ in f])\n",
    "\n",
    "\n",
    "def load_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        # +2 for UNK and EOS\n",
    "        word_ids = {line.strip(): i + 2 for i, line in enumerate(f)}\n",
    "    word_ids['<UNK>'] = 0\n",
    "    word_ids['<EOS>'] = 1\n",
    "    return word_ids\n",
    "\n",
    "\n",
    "def load_data(vocabulary, path):\n",
    "    n_lines = count_lines(path)\n",
    "    data = []\n",
    "    print('loading...: %s' % path)\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            array = numpy.array([vocabulary.get(w, UNK) for w in words], numpy.int32)\n",
    "            data.append(array)\n",
    "    return data\n",
    "\n",
    "def calculate_unknown_ratio(data):\n",
    "    unknown = sum((s == UNK).sum() for s in data)\n",
    "    total = sum(s.size for s in data)\n",
    "    return unknown / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOURCE =  \"./dataset/pncKyotoAll.en\"\n",
    "TARGET = \"./dataset/pncKyotoAll.jp\"\n",
    "SOURCE_VOCAB = \"./dataset/vocab.en\"\n",
    "TARGET_VOCAB = \"./dataset/vocab.jp\"\n",
    "validation_source =\"\" \n",
    "validation_target = \"\"\n",
    "batchsize = 5\n",
    "epoch = 20\n",
    "gpu = 0\n",
    "resume = ''\n",
    "unit = 5\n",
    "layer = 1\n",
    "min_source_sentence = 1\n",
    "max_source_sentence = 100\n",
    "min_target_sentence = 1\n",
    "min_target_sentence = 100\n",
    "log_interval = 200\n",
    "validation_interval = 4000\n",
    "out = \"result\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...: ./dataset/pncKyotoAll.en\n",
      "loading...: ./dataset/pncKyotoAll.jp\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calculate_unknown_ratio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c5b5053340a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m               \u001b[0mmin_source_sentence\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m               <= max_source_sentence]\n\u001b[0;32m---> 12\u001b[0;31m train_source_unknown = calculate_unknown_ratio(\n\u001b[0m\u001b[1;32m     13\u001b[0m     [s for s, _ in train_data])\n\u001b[1;32m     14\u001b[0m train_target_unknown = calculate_unknown_ratio(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_unknown_ratio' is not defined"
     ]
    }
   ],
   "source": [
    "source_ids = load_vocabulary(SOURCE_VOCAB)\n",
    "target_ids = load_vocabulary(TARGET_VOCAB)\n",
    "train_source = load_data(source_ids, SOURCE)\n",
    "train_target = load_data(target_ids, TARGET)\n",
    "assert len(train_source) == len(train_target)\n",
    "train_data = [(s, t)\n",
    "              for s, t in six.moves.zip(train_source, train_target)\n",
    "              if min_source_sentence <= len(s)\n",
    "              <= max_source_sentence and\n",
    "              min_source_sentence <= len(t)\n",
    "              <= max_source_sentence]\n",
    "train_source_unknown = calculate_unknown_ratio(\n",
    "    [s for s, _ in train_data])\n",
    "train_target_unknown = calculate_unknown_ratio(\n",
    "    [t for _, t in train_data])\n",
    "\n",
    "print('Source vocabulary size: %d' % len(source_ids))\n",
    "print('Target vocabulary size: %d' % len(target_ids))\n",
    "print('Train data size: %d' % len(train_data))\n",
    "print('Train source unknown ratio: %.2f%%' % (train_source_unknown * 100))\n",
    "print('Train target unknown ratio: %.2f%%' % (train_target_unknown * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_words = {i: w for w, i in target_ids.items()}\n",
    "source_words = {i: w for w, i in source_ids.items()}\n",
    "model = Seq2seq(layer, len(source_ids), len(target_ids), unit)\n",
    "if gpu >= 0:\n",
    "    chainer.cuda.get_device(gpu).use()\n",
    "    model.to_gpu(gpu)\n",
    "\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter = chainer.iterators.SerialIterator(train_data, batchsize, True, False)#shuffle=false\n",
    "updater = training.StandardUpdater(\n",
    "    train_iter, optimizer, converter=convert, device=gpu)\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "trainer.extend(extensions.LogReport(\n",
    "    trigger=(log_interval, 'iteration')))\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'iteration', 'main/loss', 'validation/main/loss',\n",
    "     'main/perp', 'validation/main/perp', 'validation/main/bleu',\n",
    "     'elapsed_time']),\n",
    "    trigger=(log_interval, 'iteration'))\n",
    "\n",
    "\n",
    "if validation_source and validation_target:\n",
    "    test_source = load_data(source_ids, validation_source)\n",
    "    test_target = load_data(target_ids, validation_target)\n",
    "    assert len(test_source) == len(test_target)\n",
    "    test_data = list(six.moves.zip(test_source, test_target))\n",
    "    test_data = [(s, t) for s, t in test_data if 0 < len(s) and 0 < len(t)]\n",
    "    test_source_unknown = calculate_unknown_ratio(\n",
    "        [s for s, _ in test_data])\n",
    "    test_target_unknown = calculate_unknown_ratio(\n",
    "        [t for _, t in test_data])\n",
    "\n",
    "    print('Validation data: %d' % len(test_data))\n",
    "    print('Validation source unknown ratio: %.2f%%' %\n",
    "          (test_source_unknown * 100))\n",
    "    print('Validation target unknown ratio: %.2f%%' %\n",
    "          (test_target_unknown * 100))\n",
    "\n",
    "    @chainer.training.make_extension()#per 1 epoch\n",
    "    def translate(trainer):\n",
    "        source, target = test_data[numpy.random.choice(len(test_data))]\n",
    "        result = model.translate([model.xp.array(source)])[0]\n",
    "\n",
    "        source_sentence = ' '.join([source_words[x] for x in source])\n",
    "        target_sentence = ' '.join([target_words[y] for y in target])\n",
    "        result_sentence = ' '.join([target_words[y] for y in result])\n",
    "        print('# source : ' + source_sentence)\n",
    "        print('#  result : ' + result_sentence)\n",
    "        print('#  expect : ' + target_sentence)\n",
    "\n",
    "    trainer.extend(\n",
    "        translate, trigger=(validation_interval, 'iteration'))\n",
    "    trainer.extend(\n",
    "        CalculateBleu(\n",
    "            model, test_data, 'validation/main/bleu', device=gpu),\n",
    "        trigger=(validation_interval, 'iteration'))\n",
    "\n",
    "print('start training')\n",
    "trainer.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
