{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "\n",
    "from nltk.translate import bleu_score\n",
    "import numpy\n",
    "#import progressbar\n",
    "import six\n",
    "import sys\n",
    "\n",
    "import chainer\n",
    "import math\n",
    "from chainer import cuda\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "UNK = 0\n",
    "EOS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_embed(embed, xs):\n",
    "    x_len = [len(x) for x in xs]\n",
    "    x_section = numpy.cumsum(x_len[:-1])\n",
    "    eee=F.concat(xs,axis=0)\n",
    "    ex = embed(F.concat(xs, axis=0))\n",
    "    exs = F.split_axis(ex, x_section, 0)\n",
    "    return exs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(batch, device):\n",
    "    def to_device_batch(batch):\n",
    "        if device is None:\n",
    "            return batch\n",
    "        elif device < 0:\n",
    "            return [chainer.dataset.to_device(device, x) for x in batch]\n",
    "        else:\n",
    "            xp = cuda.cupy.get_array_module(*batch)\n",
    "            concat = xp.concatenate(batch, axis=0)\n",
    "            sections = numpy.cumsum([len(x)\n",
    "                                     for x in batch[:-1]], dtype=numpy.int32)\n",
    "            concat_dev = chainer.dataset.to_device(device, concat)\n",
    "            batch_dev = cuda.cupy.split(concat_dev, sections)\n",
    "            return batch_dev\n",
    "\n",
    "    return {'xs': to_device_batch([x for x, _ in batch]),\n",
    "            'ys': to_device_batch([y for _, y in batch])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CalculateBleu(chainer.training.Extension):\n",
    "\n",
    "    trigger = 1, 'epoch'\n",
    "    priority = chainer.training.PRIORITY_WRITER\n",
    "\n",
    "    def __init__(self, model, test_data, key, batch=100, device=-1, max_length=100):\n",
    "        self.model = model\n",
    "        self.test_data = test_data\n",
    "        self.key = key\n",
    "        self.batch = batch\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, trainer):\n",
    "        with chainer.no_backprop_mode():\n",
    "            references = []\n",
    "            hypotheses = []\n",
    "            for i in range(0, len(self.test_data), self.batch):\n",
    "                sources, targets = zip(*self.test_data[i:i + self.batch])\n",
    "                references.extend([[t.tolist()] for t in targets])\n",
    "\n",
    "                sources = [\n",
    "                    chainer.dataset.to_device(self.device, x) for x in sources]\n",
    "                ys = [y.tolist()\n",
    "                      for y in self.model.translate(sources, self.max_length)]\n",
    "                hypotheses.extend(ys)\n",
    "\n",
    "        bleu = bleu_score.corpus_bleu(\n",
    "            references, hypotheses,\n",
    "            smoothing_function=bleu_score.SmoothingFunction().method1)\n",
    "        chainer.report({self.key: bleu})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_lines(path):\n",
    "    with open(path) as f:\n",
    "        return sum([1 for _ in f])\n",
    "\n",
    "\n",
    "def load_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        # +2 for UNK and EOS\n",
    "        word_ids = {line.strip(): i + 2 for i, line in enumerate(f)}\n",
    "    word_ids['<UNK>'] = 0\n",
    "    word_ids['<EOS>'] = 1\n",
    "    return word_ids\n",
    "\n",
    "\n",
    "def load_data(vocabulary, path):\n",
    "    n_lines = count_lines(path)\n",
    "    data = []\n",
    "    print('loading...: %s' % path)\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            array = numpy.array([vocabulary.get(w, UNK) for w in words], numpy.int32)\n",
    "            data.append(array)\n",
    "    return data\n",
    "\n",
    "def calculate_unknown_ratio(data):\n",
    "    unknown = sum((s == UNK).sum() for s in data)\n",
    "    total = sum(s.size for s in data)\n",
    "    return unknown / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOURCE =  \"./dataset/train.en\"\n",
    "TARGET = \"./dataset/train.jp\"\n",
    "SOURCE_VOCAB = \"./dataset/vocab.en\"\n",
    "TARGET_VOCAB = \"./dataset/vocab.jp\"\n",
    "validation_source =\"./dataset/test.en\" \n",
    "validation_target = \"./dataset/test.jp\"\n",
    "batchsize = 10\n",
    "epoch = 100\n",
    "gpu = -1\n",
    "resume = ''\n",
    "unit = 5\n",
    "layer = 3\n",
    "min_source_sentence = 0\n",
    "max_source_sentence = 50\n",
    "min_target_sentence = 0\n",
    "min_target_sentence = 50\n",
    "log_interval = 2\n",
    "validation_interval = 4000\n",
    "out = \"result\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...: ./dataset/train.en\n",
      "loading...: ./dataset/train.jp\n",
      "Source vocabulary size: 40002\n",
      "Target vocabulary size: 40002\n",
      "Train data size: 427910\n",
      "Train source unknown ratio: 2.17%\n",
      "Train target unknown ratio: 1.35%\n"
     ]
    }
   ],
   "source": [
    "source_ids = load_vocabulary(SOURCE_VOCAB)\n",
    "target_ids = load_vocabulary(TARGET_VOCAB)\n",
    "train_source = load_data(source_ids, SOURCE)\n",
    "train_target = load_data(target_ids, TARGET)\n",
    "assert len(train_source) == len(train_target)\n",
    "train_data = [(s, t)\n",
    "              for s, t in six.moves.zip(train_source, train_target)\n",
    "              if min_source_sentence <= len(s)\n",
    "              <= max_source_sentence and\n",
    "              min_source_sentence <= len(t)\n",
    "              <= max_source_sentence]\n",
    "train_source_unknown = calculate_unknown_ratio(\n",
    "    [s for s, _ in train_data])\n",
    "train_target_unknown = calculate_unknown_ratio(\n",
    "    [t for _, t in train_data])\n",
    "\n",
    "print('Source vocabulary size: %d' % len(source_ids))\n",
    "print('Target vocabulary size: %d' % len(target_ids))\n",
    "print('Train data size: %d' % len(train_data))\n",
    "print('Train source unknown ratio: %.2f%%' % (train_source_unknown * 100))\n",
    "print('Train target unknown ratio: %.2f%%' % (train_target_unknown * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_words = {i: w for w, i in target_ids.items()}\n",
    "source_words = {i: w for w, i in source_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_layers, n_source_vocab, n_target_vocab, n_units, batch_size):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed_x = L.EmbedID(n_source_vocab, n_units)\n",
    "            self.embed_y = L.EmbedID(n_target_vocab, n_units)\n",
    "            self.encoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            self.decoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            # add\n",
    "            self.connecter = L.Linear(None, n_units*batch_size)######################################\n",
    "            # end\n",
    "            self.W = L.Linear(n_units, n_target_vocab)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        # add##############################################################################################################################\n",
    "        self.prev_hx = chainer.Variable(numpy.array(numpy.zeros((self.n_layers, 1, self.n_units)), dtype=numpy.float32))\n",
    "        self.prev_h = chainer.Variable(numpy.array(numpy.zeros((self.n_layers, 1, self.n_units)), dtype=numpy.float32))\n",
    "        # end#################################################################################################################################\n",
    "\n",
    "    def __call__(self, xs, ys):\n",
    "        xs = [x[::-1] for x in xs]  # reverse input      [\"i\", \"am\", \"taro\"] →[\"taro\", \"am\", \"I\"]\n",
    "\n",
    "        eos = self.xp.array([EOS], numpy.int32)\n",
    "        ys_in = [F.concat([eos, y], axis=0) for y in ys]  # [eos,y1,y2,...]\n",
    "        ys_out = [F.concat([y, eos], axis=0) for y in ys]  # [y1,y2,...,eos]\n",
    "\n",
    "        exs = sequence_embed(self.embed_x, xs)\n",
    "        eys = sequence_embed(self.embed_y, ys_in)\n",
    "\n",
    "        batch = len(xs)\n",
    "\n",
    "        hx, cx, _ = self.encoder(None, None, exs)\n",
    "        #add##################################################################################################################################\n",
    "        print(\"hx.shpae = {}\".format(hx.shape))\n",
    "        forward_hx = hx[:, :-1]\n",
    "        sifted_hx = F.concat((self.prev_hx, forward_hx), axis=1)\n",
    "        in_hx = F.concat((hx, sifted_hx), axis=2)\n",
    "        out_hx = self.connecter(in_hx)\n",
    "        out_hx = out_hx.reshape(self.n_layers, -1, self.n_units)\n",
    "        print(\"out_hx = {}\".format(out_hx.shape))\n",
    "        self.prev_hx = hx[:, -1:]\n",
    "\n",
    "        '''\n",
    "        hx[0] = [[0,1,2],[3,4,5],[6,7,8]]                  shape = (layers, batch, units)\n",
    "        forward_hx[0] = [[0,1,2],[3,4,5]]                  shape = (layers, batch-1, units)\n",
    "        sifted_hx[0] = [self.prev_hx, [0,1,2],[3,4,5]]     shape = (layers, batch, units)\n",
    "        in_hx[0] = [hx[0],sifted_hx[0]]                    shape = (layers, batch, units*2)\n",
    "        out_hx[0] = connecter(in_hx)                       shape = (layers, batch, units)\n",
    "        '''\n",
    "\n",
    "        is_start_of_sentence = numpy.asarray([1 if word[-1] == 6 else 0 for word in xs]) #6 means word number of * (start of sentece)\n",
    "        is_start_of_sentence = is_start_of_sentence.reshape(-1, 1)\n",
    "        \n",
    "        new_hx = is_start_of_sentence * hx + (1 - is_start_of_sentence) * out_hx\n",
    "\n",
    "        '''\n",
    "        When\n",
    "        hx = [[[1,2,3],[4,5,6],[7,8,9]],[[11,12,13],[14,15,16],[17,18,19]],[21,22,23],[24,25,26],[27,28,29]] (shape = (layer=3, batch=3, unit=3))\n",
    "        out_hx = [[[10,20,30],[40,50,60],[70,80,90]],[[110,120,130],[140,150,160],[170,180,190]],[210,220,230],[240,250,260],[270,280,290]]\n",
    "        is_state_of_stence = [[1],[0],[1]] (shape=(batch=3, 1))\n",
    "        \n",
    "        Then, \n",
    "        new_hx = [[[1,2,3],[4,5,6],[7,8,9]],[[110,120,130],[140,150,160],[170,180,190]],[21,22,23],[24,25,26],[27,28,29]]\n",
    "        '''\n",
    "\n",
    "        hx = new_hx\n",
    "        \n",
    "        sys.exit()\n",
    "        #end############################################################################################################################\n",
    "        _, _, os = self.decoder(hx, cx, eys)\n",
    "\n",
    "        # It is faster to concatenate data before calculating loss\n",
    "        # because only one matrix multiplication is called.\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_ys_out = F.concat(ys_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_ys_out, reduce='no')) / batch\n",
    "\n",
    "        chainer.report({'loss': loss.data}, self)\n",
    "        n_words = concat_ys_out.shape[0]\n",
    "        perp = self.xp.exp(loss.data * batch / n_words)\n",
    "        chainer.report({'perp': perp}, self)\n",
    "        return loss\n",
    "\n",
    "    def translate(self, xs, max_length=50):\n",
    "        batch = len(xs)\n",
    "\n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            xs = [x[::-1] for x in xs]\n",
    "\n",
    "            exs = sequence_embed(self.embed_x, xs)\n",
    "            h, c, _ = self.encoder(None, None, exs)\n",
    "\n",
    "            # add############################################################################################################################\n",
    "            forward_h = h[:, :-1]\n",
    "            sifted_h = F.concat((self.prev_h, forward_h), axis=1)\n",
    "            in_h = F.concat((h, sifted_h), axis=2)\n",
    "            out_h = self.connecter(in_h)\n",
    "            out_h = out_h.reshape(self.n_layers, -1, self.n_units)\n",
    "            self.prev_h = h[:, -1:]\n",
    "\n",
    "            is_start_of_sentence = numpy.asarray([1 if word[-1] == 6 else 0 for word in xs])\n",
    "            is_start_of_sentence = is_start_of_sentence.reshape(-1, 1)\n",
    "\n",
    "            new_h = is_start_of_sentence * h + (1 - is_start_of_sentence) * out_h\n",
    "            h = new_h\n",
    "            # end############################################################################################################################\n",
    "            \n",
    "            ys = self.xp.full(batch, EOS, numpy.int32)\n",
    "            result = []\n",
    "            for i in range(max_length):\n",
    "                eys = self.embed_y(ys)\n",
    "                eys = F.split_axis(eys, batch, 0)\n",
    "                h, c, ys = self.decoder(h, c, eys)\n",
    "                cys = F.concat(ys, axis=0)\n",
    "                wy = self.W(cys)\n",
    "                ys = self.xp.argmax(wy.data, axis=1).astype(numpy.int32)\n",
    "                result.append(ys)\n",
    "\n",
    "        # Using `xp.concatenate(...)` instead of `xp.stack(result)` here to\n",
    "        # support NumPy 1.9.\n",
    "        result = cuda.to_cpu(\n",
    "            self.xp.concatenate([self.xp.expand_dims(x, 0) for x in result]).T)\n",
    "\n",
    "        # Remove EOS taggs\n",
    "        outs = []\n",
    "        for y in result:\n",
    "            inds = numpy.argwhere(y == EOS)\n",
    "            if len(inds) > 0:\n",
    "                y = y[:inds[0, 0]]\n",
    "            outs.append(y)\n",
    "        return outs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Seq2seq(layer, len(source_ids), len(target_ids), unit, batchsize)\n",
    "if gpu >= 0:\n",
    "    chainer.cuda.get_device(gpu).use()\n",
    "    model.to_gpu(gpu)\n",
    "\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...: ./dataset/test.en\n",
      "loading...: ./dataset/test.jp\n",
      "Validation data: 21475\n",
      "Validation source unknown ratio: 2.35%\n",
      "Validation target unknown ratio: 1.34%\n",
      "start training\n",
      "hx.shpae = (3, 10, 5)\n",
      "out_hx = (3, 10, 5)\n",
      "hx = variable([[-0.00710593 -0.11615726 -0.29196703 -0.04248418 -0.31688586]\n",
      "          [-0.02959079 -0.00991984 -0.26676309 -0.18179855 -0.39400068]\n",
      "          [ 0.00122895 -0.08223131 -0.19211401 -0.025917   -0.1504125 ]\n",
      "          [-0.07083362 -0.01757192 -0.4188399  -0.28636593 -0.5799706 ]\n",
      "          [-0.11428204 -0.03555323 -0.51421869 -0.47688594 -0.6587736 ]\n",
      "          [-0.09257188 -0.00259989 -0.09312538  0.15563785 -0.2350609 ]\n",
      "          [-0.07991023 -0.0946406  -0.3699441  -0.35866824 -0.64881027]\n",
      "          [-0.14462504 -0.06860535 -0.28551117  0.0511317  -0.22189814]\n",
      "          [-0.06495135  0.16120255  0.04248552  0.40413541  0.03993991]\n",
      "          [-0.10775705 -0.01252826 -0.26350296 -0.07304016 -0.51471913]])\n",
      "out_hx = variable([[ 0.25969955 -0.53675395  0.62799299  0.6987226   0.115751  ]\n",
      "          [-0.0561886  -0.72856867  0.11294891  0.12851426 -0.11104997]\n",
      "          [ 0.24490377 -0.44814131  0.13987188 -0.12013626  0.10009389]\n",
      "          [ 0.18718779  0.03333407 -0.0446882  -0.45628631 -0.28773409]\n",
      "          [-0.27425653 -0.7331425   0.37111083 -0.24169582  0.22633253]\n",
      "          [-0.21202482 -0.07913543  0.43449172 -0.60784131 -0.08779109]\n",
      "          [-0.14537838 -0.0672826   0.17433517  0.27975231  0.12035716]\n",
      "          [ 0.03807722 -0.25500348  0.37075311  0.44798008 -0.23589751]\n",
      "          [ 0.0285233   0.17684886 -0.12835082  0.26906702  0.08337199]\n",
      "          [-0.49506289 -0.07322708  0.00090489 -0.23098207 -0.11119303]])\n",
      "new_hx = variable([[-0.00710593 -0.11615726 -0.29196703 -0.04248418 -0.31688586]\n",
      "          [-0.02959079 -0.00991984 -0.26676309 -0.18179855 -0.39400068]\n",
      "          [ 0.00122895 -0.08223131 -0.19211401 -0.025917   -0.1504125 ]\n",
      "          [-0.07083362 -0.01757192 -0.4188399  -0.28636593 -0.5799706 ]\n",
      "          [-0.11428204 -0.03555323 -0.51421869 -0.47688594 -0.6587736 ]\n",
      "          [-0.09257188 -0.00259989 -0.09312538  0.15563785 -0.2350609 ]\n",
      "          [-0.07991023 -0.0946406  -0.3699441  -0.35866824 -0.64881027]\n",
      "          [-0.14462504 -0.06860535 -0.28551117  0.0511317  -0.22189814]\n",
      "          [ 0.0285233   0.17684886 -0.12835082  0.26906702  0.08337199]\n",
      "          [-0.10775705 -0.01252826 -0.26350296 -0.07304016 -0.51471913]])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "train_iter = chainer.iterators.SerialIterator(train_data, batchsize, True, False)#shuffle=false\n",
    "updater = training.StandardUpdater(\n",
    "    train_iter, optimizer, converter=convert, device=gpu)\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "trainer.extend(extensions.LogReport(\n",
    "    trigger=(log_interval, 'iteration')))\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'iteration', 'main/loss', 'validation/main/loss',\n",
    "     'main/perp', 'validation/main/perp', 'validation/main/bleu',\n",
    "     'elapsed_time']),\n",
    "    trigger=(log_interval, 'iteration'))\n",
    "\n",
    "\n",
    "if validation_source and validation_target:\n",
    "    test_source = load_data(source_ids, validation_source)\n",
    "    test_target = load_data(target_ids, validation_target)\n",
    "    assert len(test_source) == len(test_target)\n",
    "    test_data = list(six.moves.zip(test_source, test_target))\n",
    "    test_data = [(s, t) for s, t in test_data if 0 < len(s) and 0 < len(t)]\n",
    "    test_source_unknown = calculate_unknown_ratio(\n",
    "        [s for s, _ in test_data])\n",
    "    test_target_unknown = calculate_unknown_ratio(\n",
    "        [t for _, t in test_data])\n",
    "\n",
    "    print('Validation data: %d' % len(test_data))\n",
    "    print('Validation source unknown ratio: %.2f%%' %\n",
    "          (test_source_unknown * 100))\n",
    "    print('Validation target unknown ratio: %.2f%%' %\n",
    "          (test_target_unknown * 100))\n",
    "\n",
    "    @chainer.training.make_extension()#per 1 epoch\n",
    "    def translate(trainer):\n",
    "        source, target = test_data[numpy.random.choice(len(test_data))]\n",
    "        result = model.translate([model.xp.array(source)])[0]\n",
    "\n",
    "        source_sentence = ' '.join([source_words[x] for x in source])\n",
    "        target_sentence = ' '.join([target_words[y] for y in target])\n",
    "        result_sentence = ' '.join([target_words[y] for y in result])\n",
    "        print('# source : ' + source_sentence)\n",
    "        print('#  result : ' + result_sentence)\n",
    "        print('#  expect : ' + target_sentence)\n",
    "\n",
    "    trainer.extend(\n",
    "        translate, trigger=(validation_interval, 'iteration'))\n",
    "    trainer.extend(\n",
    "        CalculateBleu(\n",
    "            model, test_data, 'validation/main/bleu', device=gpu),\n",
    "        trigger=(validation_interval, 'iteration'))\n",
    "\n",
    "print('start training')\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = numpy.arange(60).reshape(3,4,5) #(layer, batch, unit)\n",
    "b = numpy.arange(100, 160).reshape(3,4,5)\n",
    "fora = a[:, :-1]\n",
    "lastb = b[:, -1:]\n",
    "stacked = numpy.hstack((lastb,fora))\n",
    "# print(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = numpy.arange(60).reshape(3,4,5)  #(layer, batch, unit)\n",
    "b = numpy.arange(100, 160).reshape(3,4,5)\n",
    "z = F.concat((a, b), axis=2)\n",
    "# print(z.shape)\n",
    "# print(z.data)\n",
    "# print(type(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = numpy.arange(60).reshape(3,4,5) #(layer, batch, unit)\n",
    "b = numpy.arange(100, 160).reshape(3,4,5)\n",
    "xs = numpy.asarray([[1,2,3,4,5,0], [3,2,1,56,7,2], [1,2,3,4,1,1], [0,3,3,5,1,0]])\n",
    "c = numpy.asarray([1 if i[-1] == 0 else 0 for i in xs])\n",
    "c = c.reshape(-1, 1)\n",
    "#print(c*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 7), (1, 2), (3, 2), (8, 9)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-7c5f829f87e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "l = []\n",
    "for i in range(4):\n",
    "    s = random.randint(0, 10)\n",
    "    t = random.randint(0, 10)    \n",
    "    l.append((s,t))\n",
    "print(l)\n",
    "source = []\n",
    "target = []\n",
    "for i in l:\n",
    "    source.append(i[0])\n",
    "    target.append(i[1])\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_layers, n_source_vocab, n_target_vocab, n_units):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed_x = L.EmbedID(n_source_vocab, n_units)\n",
    "            self.embed_y = L.EmbedID(n_target_vocab, n_units)\n",
    "            self.encoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            self.decoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            # add\n",
    "            self.connecter = L.Linear(None, n_units)\n",
    "            # end\n",
    "            self.W = L.Linear(n_units, n_target_vocab)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        # add\n",
    "        self.prev_hx = None\n",
    "        self.prev_h = None\n",
    "        # end\n",
    "\n",
    "    def __call__(self, xs, ys):\n",
    "        xs = [x[::-1] for x in xs]  # reverse input      [\"i\", \"am\", \"taro\"] →[\"taro\", \"am\", \"I\"]\n",
    "\n",
    "        eos = self.xp.array([EOS], numpy.int32)\n",
    "        ys_in = [F.concat([eos, y], axis=0) for y in ys]  # [eos,y1,y2,...]\n",
    "        ys_out = [F.concat([y, eos], axis=0) for y in ys]  # [y1,y2,...,eos]\n",
    "\n",
    "        exs = sequence_embed(self.embed_x, xs)\n",
    "        eys = sequence_embed(self.embed_y, ys_in)\n",
    "\n",
    "        batch = len(xs)\n",
    "\n",
    "        hx, cx, _ = self.encoder(None, None, exs)\n",
    "        # add ############################################################################################################\n",
    "        self.prev_hx = hx\n",
    "\n",
    "        if xs[0][-1] != 6 and self.prev_hx is not None:\n",
    "            #print(\"connect!\")\n",
    "            hx = chainer.functions.concat([hx, self.prev_hx], axis=1).data\n",
    "            hx = self.connecter(hx)\n",
    "            hx = F.reshape(hx, (self.n_layers, batch, self.n_units))  # (3, 1, 500)\n",
    "\n",
    "        # end############################################################################################################\n",
    "        _, _, os = self.decoder(hx, cx, eys)\n",
    "\n",
    "        # It is faster to concatenate data before calculating loss\n",
    "        # because only one matrix multiplication is called.\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_ys_out = F.concat(ys_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_ys_out, reduce='no')) / batch\n",
    "\n",
    "        chainer.report({'loss': loss.data}, self)\n",
    "        n_words = concat_ys_out.shape[0]\n",
    "        perp = self.xp.exp(loss.data * batch / n_words)\n",
    "        chainer.report({'perp': perp}, self)\n",
    "        return loss\n",
    "\n",
    "    def translate(self, xs, max_length=50):\n",
    "        batch = len(xs)\n",
    "\n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            xs = [x[::-1] for x in xs]\n",
    "\n",
    "            exs = sequence_embed(self.embed_x, xs)\n",
    "            h, c, _ = self.encoder(None, None, exs)\n",
    "\n",
    "            # add\n",
    "            self.prev_h = h\n",
    "            if xs[0][-1] != 6 and self.prev_h is not None:\n",
    "                h = chainer.functions.concat([h, self.prev_h], axis=1).data\n",
    "                h = self.connecter(h)\n",
    "                h = F.reshape(h, (self.n_layers, batch, self.n_units))  # (3, 1, 500)\n",
    "            # end\n",
    "            \n",
    "            ys = self.xp.full(batch, EOS, numpy.int32)\n",
    "            result = []\n",
    "            for i in range(max_length):\n",
    "                eys = self.embed_y(ys)\n",
    "                eys = F.split_axis(eys, batch, 0)\n",
    "                h, c, ys = self.decoder(h, c, eys)\n",
    "                cys = F.concat(ys, axis=0)\n",
    "                wy = self.W(cys)\n",
    "                ys = self.xp.argmax(wy.data, axis=1).astype(numpy.int32)\n",
    "                result.append(ys)\n",
    "\n",
    "        # Using `xp.concatenate(...)` instead of `xp.stack(result)` here to\n",
    "        # support NumPy 1.9.\n",
    "        result = cuda.to_cpu(\n",
    "            self.xp.concatenate([self.xp.expand_dims(x, 0) for x in result]).T)\n",
    "\n",
    "        # Remove EOS taggs\n",
    "        outs = []\n",
    "        for y in result:\n",
    "            inds = numpy.argwhere(y == EOS)\n",
    "            if len(inds) > 0:\n",
    "                y = y[:inds[0, 0]]\n",
    "            outs.append(y)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
