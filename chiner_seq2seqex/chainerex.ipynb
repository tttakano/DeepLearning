{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "\n",
    "from nltk.translate import bleu_score\n",
    "import numpy\n",
    "#import progressbar\n",
    "import six\n",
    "import sys\n",
    "\n",
    "import chainer\n",
    "import math\n",
    "from chainer import cuda\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "UNK = 0\n",
    "EOS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_embed(embed, xs):\n",
    "    x_len = [len(x) for x in xs]\n",
    "    x_section = numpy.cumsum(x_len[:-1])\n",
    "    eee=F.concat(xs,axis=0)\n",
    "    ex = embed(F.concat(xs, axis=0))\n",
    "    exs = F.split_axis(ex, x_section, 0)\n",
    "    return exs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(batch, device):\n",
    "    def to_device_batch(batch):\n",
    "        if device is None:\n",
    "            return batch\n",
    "        elif device < 0:\n",
    "            return [chainer.dataset.to_device(device, x) for x in batch]\n",
    "        else:\n",
    "            xp = cuda.cupy.get_array_module(*batch)\n",
    "            concat = xp.concatenate(batch, axis=0)\n",
    "            sections = numpy.cumsum([len(x)\n",
    "                                     for x in batch[:-1]], dtype=numpy.int32)\n",
    "            concat_dev = chainer.dataset.to_device(device, concat)\n",
    "            batch_dev = cuda.cupy.split(concat_dev, sections)\n",
    "            return batch_dev\n",
    "\n",
    "    return {'xs': to_device_batch([x for x, _ in batch]),\n",
    "            'ys': to_device_batch([y for _, y in batch])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CalculateBleu(chainer.training.Extension):\n",
    "\n",
    "    trigger = 1, 'epoch'\n",
    "    priority = chainer.training.PRIORITY_WRITER\n",
    "\n",
    "    def __init__(self, model, test_data, key, batch=100, device=-1, max_length=100):\n",
    "        self.model = model\n",
    "        self.test_data = test_data\n",
    "        self.key = key\n",
    "        self.batch = batch\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, trainer):\n",
    "        with chainer.no_backprop_mode():\n",
    "            references = []\n",
    "            hypotheses = []\n",
    "            for i in range(0, len(self.test_data), self.batch):\n",
    "                sources, targets = zip(*self.test_data[i:i + self.batch])\n",
    "                references.extend([[t.tolist()] for t in targets])\n",
    "\n",
    "                sources = [\n",
    "                    chainer.dataset.to_device(self.device, x) for x in sources]\n",
    "                ys = [y.tolist()\n",
    "                      for y in self.model.translate(sources, self.max_length)]\n",
    "                hypotheses.extend(ys)\n",
    "\n",
    "        bleu = bleu_score.corpus_bleu(\n",
    "            references, hypotheses,\n",
    "            smoothing_function=bleu_score.SmoothingFunction().method1)\n",
    "        chainer.report({self.key: bleu})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_lines(path):\n",
    "    with open(path) as f:\n",
    "        return sum([1 for _ in f])\n",
    "\n",
    "\n",
    "def load_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        # +2 for UNK and EOS\n",
    "        word_ids = {line.strip(): i + 2 for i, line in enumerate(f)}\n",
    "    word_ids['<UNK>'] = 0\n",
    "    word_ids['<EOS>'] = 1\n",
    "    return word_ids\n",
    "\n",
    "\n",
    "def load_data(vocabulary, path):\n",
    "    n_lines = count_lines(path)\n",
    "    data = []\n",
    "    print('loading...: %s' % path)\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            array = numpy.array([vocabulary.get(w, UNK) for w in words], numpy.int32)\n",
    "            data.append(array)\n",
    "    return data\n",
    "\n",
    "def calculate_unknown_ratio(data):\n",
    "    unknown = sum((s == UNK).sum() for s in data)\n",
    "    total = sum(s.size for s in data)\n",
    "    return unknown / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SOURCE =  \"./dataset/train.en\"\n",
    "TARGET = \"./dataset/train.jp\"\n",
    "SOURCE_VOCAB = \"./dataset/vocab.en\"\n",
    "TARGET_VOCAB = \"./dataset/vocab.jp\"\n",
    "validation_source =\"./dataset/test.en\" \n",
    "validation_target = \"./dataset/test.jp\"\n",
    "batchsize = 1\n",
    "epoch = 100\n",
    "gpu = -1\n",
    "resume = ''\n",
    "unit = 500\n",
    "layer = 3\n",
    "min_source_sentence = 0\n",
    "max_source_sentence = 50\n",
    "min_target_sentence = 0\n",
    "min_target_sentence = 50\n",
    "log_interval = 2\n",
    "validation_interval = 4000\n",
    "out = \"result\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...: ./dataset/train.en\n",
      "loading...: ./dataset/train.jp\n",
      "Source vocabulary size: 40002\n",
      "Target vocabulary size: 40002\n",
      "Train data size: 427910\n",
      "Train source unknown ratio: 2.17%\n",
      "Train target unknown ratio: 1.35%\n"
     ]
    }
   ],
   "source": [
    "source_ids = load_vocabulary(SOURCE_VOCAB)\n",
    "target_ids = load_vocabulary(TARGET_VOCAB)\n",
    "train_source = load_data(source_ids, SOURCE)\n",
    "train_target = load_data(target_ids, TARGET)\n",
    "assert len(train_source) == len(train_target)\n",
    "train_data = [(s, t)\n",
    "              for s, t in six.moves.zip(train_source, train_target)\n",
    "              if min_source_sentence <= len(s)\n",
    "              <= max_source_sentence and\n",
    "              min_source_sentence <= len(t)\n",
    "              <= max_source_sentence]\n",
    "train_source_unknown = calculate_unknown_ratio(\n",
    "    [s for s, _ in train_data])\n",
    "train_target_unknown = calculate_unknown_ratio(\n",
    "    [t for _, t in train_data])\n",
    "\n",
    "print('Source vocabulary size: %d' % len(source_ids))\n",
    "print('Target vocabulary size: %d' % len(target_ids))\n",
    "print('Train data size: %d' % len(train_data))\n",
    "print('Train source unknown ratio: %.2f%%' % (train_source_unknown * 100))\n",
    "print('Train target unknown ratio: %.2f%%' % (train_target_unknown * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_words = {i: w for w, i in target_ids.items()}\n",
    "source_words = {i: w for w, i in source_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_layers, n_source_vocab, n_target_vocab, n_units):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed_x = L.EmbedID(n_source_vocab, n_units)\n",
    "            self.embed_y = L.EmbedID(n_target_vocab, n_units)\n",
    "            self.encoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            self.decoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            # add\n",
    "            self.connecter = L.Linear(None, n_units)\n",
    "            # end\n",
    "            self.W = L.Linear(n_units, n_target_vocab)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "        # add\n",
    "        self.prev_hx = None\n",
    "        self.prev_h = None\n",
    "        # end\n",
    "\n",
    "    def __call__(self, xs, ys):\n",
    "        xs = [x[::-1] for x in xs]  # reverse input      [\"i\", \"am\", \"taro\"] â†’[\"taro\", \"am\", \"I\"]\n",
    "\n",
    "        eos = self.xp.array([EOS], numpy.int32)\n",
    "        ys_in = [F.concat([eos, y], axis=0) for y in ys]  # [eos,y1,y2,...]\n",
    "        ys_out = [F.concat([y, eos], axis=0) for y in ys]  # [y1,y2,...,eos]\n",
    "\n",
    "        exs = sequence_embed(self.embed_x, xs)\n",
    "        eys = sequence_embed(self.embed_y, ys_in)\n",
    "\n",
    "        batch = len(xs)\n",
    "\n",
    "        hx, cx, _ = self.encoder(None, None, exs)\n",
    "        # add ############################################################################################################\n",
    "        self.prev_hx = hx\n",
    "\n",
    "        if xs[0][-1] != 6 and self.prev_hx is not None:\n",
    "            #print(\"connect!\")\n",
    "            hx = chainer.functions.concat([hx, self.prev_hx], axis=1).data\n",
    "            hx = self.connecter(hx)\n",
    "            hx = F.reshape(hx, (self.n_layers, batch, self.n_units))  # (3, 1, 500)\n",
    "\n",
    "        # end############################################################################################################\n",
    "        _, _, os = self.decoder(hx, cx, eys)\n",
    "\n",
    "        # It is faster to concatenate data before calculating loss\n",
    "        # because only one matrix multiplication is called.\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_ys_out = F.concat(ys_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_ys_out, reduce='no')) / batch\n",
    "\n",
    "        chainer.report({'loss': loss.data}, self)\n",
    "        n_words = concat_ys_out.shape[0]\n",
    "        perp = self.xp.exp(loss.data * batch / n_words)\n",
    "        chainer.report({'perp': perp}, self)\n",
    "        return loss\n",
    "\n",
    "    def translate(self, xs, max_length=50):\n",
    "        batch = len(xs)\n",
    "\n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            xs = [x[::-1] for x in xs]\n",
    "\n",
    "            exs = sequence_embed(self.embed_x, xs)\n",
    "            h, c, _ = self.encoder(None, None, exs)\n",
    "\n",
    "            # add\n",
    "            self.prev_h = h\n",
    "            if xs[0][-1] != 6 and self.prev_h is not None:\n",
    "                h = chainer.functions.concat([h, self.prev_h], axis=1).data\n",
    "                h = self.connecter(h)\n",
    "                h = F.reshape(h, (self.n_layers, batch, self.n_units))  # (3, 1, 500)\n",
    "            # end\n",
    "            \n",
    "            ys = self.xp.full(batch, EOS, numpy.int32)\n",
    "            result = []\n",
    "            for i in range(max_length):\n",
    "                eys = self.embed_y(ys)\n",
    "                eys = F.split_axis(eys, batch, 0)\n",
    "                h, c, ys = self.decoder(h, c, eys)\n",
    "                cys = F.concat(ys, axis=0)\n",
    "                wy = self.W(cys)\n",
    "                ys = self.xp.argmax(wy.data, axis=1).astype(numpy.int32)\n",
    "                result.append(ys)\n",
    "\n",
    "        # Using `xp.concatenate(...)` instead of `xp.stack(result)` here to\n",
    "        # support NumPy 1.9.\n",
    "        result = cuda.to_cpu(\n",
    "            self.xp.concatenate([self.xp.expand_dims(x, 0) for x in result]).T)\n",
    "\n",
    "        # Remove EOS taggs\n",
    "        outs = []\n",
    "        for y in result:\n",
    "            inds = numpy.argwhere(y == EOS)\n",
    "            if len(inds) > 0:\n",
    "                y = y[:inds[0, 0]]\n",
    "            outs.append(y)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Seq2seq(layer, len(source_ids), len(target_ids), unit)\n",
    "if gpu >= 0:\n",
    "    chainer.cuda.get_device(gpu).use()\n",
    "    model.to_gpu(gpu)\n",
    "\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...: ./dataset/test.en\n",
      "loading...: ./dataset/test.jp\n",
      "Validation data: 21475\n",
      "Validation source unknown ratio: 2.35%\n",
      "Validation target unknown ratio: 1.34%\n",
      "start training\n",
      "epoch       iteration   main/loss   validation/main/loss  main/perp   validation/main/perp  validation/main/bleu  elapsed_time\n",
      "\u001b[J0           2           31.4018                           35441.8                                                 4.49662       \n",
      "\u001b[J0           4           55.5456                           23552.9                                                 8.65341       \n",
      "\u001b[J0           6           39.7472                           6684.26                                                 12.6475       \n",
      "\u001b[J0           8           295.067                           59316.8                                                 21.787        \n",
      "\u001b[J0           10          189.03                            25470.9                                                 28.1825       \n",
      "\u001b[J0           12          218.375                           32977.3                                                 36.1538       \n",
      "\u001b[J0           14          214.395                           17740.9                                                 42.8069       \n",
      "\u001b[J0           16          263.911                           14714.3                                                 52.3273       \n",
      "\u001b[J0           18          287.529                           10063.5                                                 62.3198       \n",
      "\u001b[J0           20          139.274                           5166.47                                                 69.3016       \n",
      "\u001b[J0           22          126.442                           8658.37                                                 75.1315       \n",
      "\u001b[J0           24          287.838                           4037.52                                                 86.0489       \n",
      "\u001b[J0           26          208.657                           2672.91                                                 95.1642       \n",
      "\u001b[J0           28          193.941                           1480.51                                                 104.176       \n",
      "\u001b[J0           30          108.384                           934.931                                                 111.702       \n",
      "\u001b[J0           32          82.4622                           440.078                                                 118.1         \n",
      "\u001b[J0           34          96.2001                           903.098                                                 124.162       \n",
      "\u001b[J0           36          307.366                           1408.7                                                  136.335       \n",
      "\u001b[J0           38          154.658                           1396.99                                                 143.629       \n",
      "\u001b[J0           40          147.285                           365.739                                                 152.07        \n",
      "\u001b[J0           42          131.191                           1410.17                                                 159.322       \n",
      "\u001b[J0           44          137.665                           638.304                                                 166.573       \n",
      "\u001b[J0           46          120.829                           895.694                                                 172.452       \n",
      "\u001b[J0           48          103.021                           1163.36                                                 178.577       \n",
      "\u001b[J0           50          126.122                           472.722                                                 185.797       \n",
      "\u001b[J0           52          55.1638                           690.115                                                 190.667       \n",
      "\u001b[J0           54          66.1921                           611.182                                                 196.136       \n",
      "\u001b[J0           56          71.0876                           885.781                                                 201.945       \n",
      "\u001b[J0           58          66.553                            574.813                                                 207.123       \n",
      "\u001b[J0           60          48.192                            285.996                                                 211.982       \n",
      "\u001b[J0           62          80.2916                           2028.74                                                 217.301       \n",
      "\u001b[J0           64          36.9547                           224.558                                                 221.579       \n",
      "\u001b[J0           66          68.4752                           1322.15                                                 227.122       \n",
      "\u001b[J0           68          222.479                           1538.18                                                 235.858       \n",
      "\u001b[J0           70          47.3394                           318.924                                                 240.865       \n",
      "\u001b[J0           72          274.581                           2557.84                                                 252.76        \n",
      "\u001b[J0           74          126.789                           2785.56                                                 259.288       \n",
      "\u001b[J0           76          231.484                           1697.88                                                 269.199       \n",
      "\u001b[J0           78          143.303                           983.67                                                  276.716       \n",
      "\u001b[J0           80          99.3986                           444.951                                                 283.857       \n",
      "\u001b[J0           82          240.853                           2926.59                                                 294.533       \n",
      "\u001b[J0           84          134.54                            659.569                                                 300.849       \n",
      "\u001b[J0           86          171.968                           1253.6                                                  310.016       \n",
      "\u001b[J0           88          274.954                           1385.95                                                 320.952       \n"
     ]
    }
   ],
   "source": [
    "train_iter = chainer.iterators.SerialIterator(train_data, batchsize, True, False)#shuffle=false\n",
    "updater = training.StandardUpdater(\n",
    "    train_iter, optimizer, converter=convert, device=gpu)\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "trainer.extend(extensions.LogReport(\n",
    "    trigger=(log_interval, 'iteration')))\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'iteration', 'main/loss', 'validation/main/loss',\n",
    "     'main/perp', 'validation/main/perp', 'validation/main/bleu',\n",
    "     'elapsed_time']),\n",
    "    trigger=(log_interval, 'iteration'))\n",
    "\n",
    "\n",
    "if validation_source and validation_target:\n",
    "    test_source = load_data(source_ids, validation_source)\n",
    "    test_target = load_data(target_ids, validation_target)\n",
    "    assert len(test_source) == len(test_target)\n",
    "    test_data = list(six.moves.zip(test_source, test_target))\n",
    "    test_data = [(s, t) for s, t in test_data if 0 < len(s) and 0 < len(t)]\n",
    "    test_source_unknown = calculate_unknown_ratio(\n",
    "        [s for s, _ in test_data])\n",
    "    test_target_unknown = calculate_unknown_ratio(\n",
    "        [t for _, t in test_data])\n",
    "\n",
    "    print('Validation data: %d' % len(test_data))\n",
    "    print('Validation source unknown ratio: %.2f%%' %\n",
    "          (test_source_unknown * 100))\n",
    "    print('Validation target unknown ratio: %.2f%%' %\n",
    "          (test_target_unknown * 100))\n",
    "\n",
    "    @chainer.training.make_extension()#per 1 epoch\n",
    "    def translate(trainer):\n",
    "        source, target = test_data[numpy.random.choice(len(test_data))]\n",
    "        result = model.translate([model.xp.array(source)])[0]\n",
    "\n",
    "        source_sentence = ' '.join([source_words[x] for x in source])\n",
    "        target_sentence = ' '.join([target_words[y] for y in target])\n",
    "        result_sentence = ' '.join([target_words[y] for y in result])\n",
    "        print('# source : ' + source_sentence)\n",
    "        print('#  result : ' + result_sentence)\n",
    "        print('#  expect : ' + target_sentence)\n",
    "\n",
    "    trainer.extend(\n",
    "        translate, trigger=(validation_interval, 'iteration'))\n",
    "    trainer.extend(\n",
    "        CalculateBleu(\n",
    "            model, test_data, 'validation/main/bleu', device=gpu),\n",
    "        trigger=(validation_interval, 'iteration'))\n",
    "\n",
    "print('start training')\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3)\n",
      "(3, 2, 3)\n",
      "[[[1 2 3]\n",
      "  [1 2 3]\n",
      "  [9 8 7]\n",
      "  [9 8 7]]\n",
      "\n",
      " [[4 5 6]\n",
      "  [4 5 6]\n",
      "  [6 5 4]\n",
      "  [6 5 4]]\n",
      "\n",
      " [[7 8 9]\n",
      "  [7 8 9]\n",
      "  [3 2 1]\n",
      "  [3 2 1]]]\n",
      "(3, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "tx = numpy.array([[[1,2,3], [1,2,3]],[[4,5,6], [4,5,6]],[[7,8,9], [7,8,9]]])\n",
    "print(tx.shape)\n",
    "px = numpy.array([[[9,8,7], [9,8,7]],[[6,5,4], [6,5,4]],[[3,2,1], [3,2,1]]])\n",
    "print(px.shape)\n",
    "cx = numpy.concatenate((tx, px), axis=1)\n",
    "print(cx)\n",
    "print(cx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
