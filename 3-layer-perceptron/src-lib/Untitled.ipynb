{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=100\n",
    "n_epoch=500\n",
    "df=pd.read_csv('../dataset/sample.csv')\n",
    "x_train=df['x']\n",
    "x_test=df['x']\n",
    "y_train=df['y']\n",
    "y_test=df['y_org']\n",
    "x_train=np.array(x_train).astype(np.float32).reshape(len(x_train),1)\n",
    "x_test=np.array(x_test).astype(np.float32).reshape(len(x_test),1)\n",
    "y_train=np.array(y_train).astype(np.float32).reshape(len(y_train),1)\n",
    "y_test=np.array(y_test).astype(np.float32).reshape(len(y_test),1)\n",
    "N = len(y_train)\n",
    "N_test = len(y_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyModel (Chain):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__(\n",
    "            fc1=L.Linear(None,n_node),\n",
    "            fc2=L.Linear(None,n_node),\n",
    "            fc3=L.Linear(None,1),\n",
    "        )\n",
    "        \n",
    "    def __call__(self,x_data,y_data,fi,train=True):\n",
    "        x = chainer.Variable(x_data, volatile=not train)\n",
    "        t = chainer.Variable(y_data, volatile=not train)\n",
    "        \n",
    "        h = F.concat((x, fi))\n",
    "        h = F.dropout(F.relu(self.fc1(h)), train=train)\n",
    "        h = F.concat((h, fi))\n",
    "        h = F.dropout(F.relu(self.fc2(h)), train=train)\n",
    "        h = self.fc3(h)\n",
    " \n",
    "        return F.mean_squared_error(h, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      " train mean loss=6.168666002750396\n",
      "epoch 2\n",
      " train mean loss=0.323932833224535\n",
      "epoch 3\n",
      " train mean loss=0.2305823066830635\n",
      "epoch 4\n",
      " train mean loss=0.17623971045017242\n",
      "epoch 5\n",
      " train mean loss=0.15114615961909295\n",
      "epoch 6\n",
      " train mean loss=0.12821440406143667\n",
      "epoch 7\n",
      " train mean loss=0.1128974599391222\n",
      "epoch 8\n",
      " train mean loss=0.10117179967463016\n",
      "epoch 9\n",
      " train mean loss=0.09193356666713953\n",
      "epoch 10\n",
      " train mean loss=0.08036025498062373\n",
      "epoch 11\n",
      " train mean loss=0.07298236805945635\n",
      "epoch 12\n",
      " train mean loss=0.06861823558807373\n",
      "epoch 13\n",
      " train mean loss=0.0632687245309353\n",
      "epoch 14\n",
      " train mean loss=0.058622557260096075\n",
      "epoch 15\n",
      " train mean loss=0.05534679926931858\n",
      "epoch 16\n",
      " train mean loss=0.0519487813487649\n",
      "epoch 17\n",
      " train mean loss=0.04847261052578688\n",
      "epoch 18\n",
      " train mean loss=0.047592074684798716\n",
      "epoch 19\n",
      " train mean loss=0.047152758166193964\n",
      "epoch 20\n",
      " train mean loss=0.042633745819330215\n",
      "epoch 21\n",
      " train mean loss=0.04341734599322081\n",
      "epoch 22\n",
      " train mean loss=0.040751111879944804\n",
      "epoch 23\n",
      " train mean loss=0.04040898585692048\n",
      "epoch 24\n",
      " train mean loss=0.039138365685939785\n",
      "epoch 25\n",
      " train mean loss=0.03622244205325842\n",
      "epoch 26\n",
      " train mean loss=0.035954198874533175\n",
      "epoch 27\n",
      " train mean loss=0.034770887587219475\n",
      "epoch 28\n",
      " train mean loss=0.03377675712108612\n",
      "epoch 29\n",
      " train mean loss=0.03372068155556917\n",
      "epoch 30\n",
      " train mean loss=0.0322816214337945\n",
      "epoch 31\n",
      " train mean loss=0.03228698253631592\n",
      "epoch 32\n",
      " train mean loss=0.031108852997422217\n",
      "epoch 33\n",
      " train mean loss=0.03169981649145484\n",
      "epoch 34\n",
      " train mean loss=0.02933003231883049\n",
      "epoch 35\n",
      " train mean loss=0.02929584516212344\n",
      "epoch 36\n",
      " train mean loss=0.029936133064329625\n",
      "epoch 37\n",
      " train mean loss=0.028302765805274247\n",
      "epoch 38\n",
      " train mean loss=0.02790500419214368\n",
      "epoch 39\n",
      " train mean loss=0.028423868082463742\n",
      "epoch 40\n",
      " train mean loss=0.027657704893499613\n",
      "epoch 41\n",
      " train mean loss=0.027083827443420888\n",
      "epoch 42\n",
      " train mean loss=0.026721924766898154\n",
      "epoch 43\n",
      " train mean loss=0.026272371970117094\n",
      "epoch 44\n",
      " train mean loss=0.02649549029767513\n",
      "epoch 45\n",
      " train mean loss=0.026088983546942474\n",
      "epoch 46\n",
      " train mean loss=0.026877540405839683\n",
      "epoch 47\n",
      " train mean loss=0.02600781189277768\n",
      "epoch 48\n",
      " train mean loss=0.026333922687917947\n",
      "epoch 49\n",
      " train mean loss=0.02441623106598854\n",
      "epoch 50\n",
      " train mean loss=0.02473507734015584\n",
      "epoch 51\n",
      " train mean loss=0.024719805885106326\n",
      "epoch 52\n",
      " train mean loss=0.023976995348930358\n",
      "epoch 53\n",
      " train mean loss=0.024109137384220956\n",
      "epoch 54\n",
      " train mean loss=0.023900165650993586\n",
      "epoch 55\n",
      " train mean loss=0.024316089637577535\n",
      "epoch 56\n",
      " train mean loss=0.02363177272491157\n",
      "epoch 57\n",
      " train mean loss=0.02373739978298545\n",
      "epoch 58\n",
      " train mean loss=0.023829510658979414\n",
      "epoch 59\n",
      " train mean loss=0.023951808679848908\n",
      "epoch 60\n",
      " train mean loss=0.02304893419146538\n",
      "epoch 61\n",
      " train mean loss=0.023473366368561982\n",
      "epoch 62\n",
      " train mean loss=0.023131667096167804\n",
      "epoch 63\n",
      " train mean loss=0.022074675578624012\n",
      "epoch 64\n",
      " train mean loss=0.02275753790512681\n",
      "epoch 65\n",
      " train mean loss=0.022740305215120316\n",
      "epoch 66\n",
      " train mean loss=0.022591468952596186\n",
      "epoch 67\n",
      " train mean loss=0.02287866150960326\n",
      "epoch 68\n",
      " train mean loss=0.0222035791631788\n",
      "epoch 69\n",
      " train mean loss=0.021043090242892505\n",
      "epoch 70\n",
      " train mean loss=0.022814076216891407\n",
      "epoch 71\n",
      " train mean loss=0.02164994703605771\n",
      "epoch 72\n",
      " train mean loss=0.0216140076238662\n",
      "epoch 73\n",
      " train mean loss=0.02082459201104939\n",
      "epoch 74\n",
      " train mean loss=0.021622637836262584\n",
      "epoch 75\n",
      " train mean loss=0.021427619103342295\n",
      "epoch 76\n",
      " train mean loss=0.02161963746882975\n",
      "epoch 77\n",
      " train mean loss=0.02095854370854795\n",
      "epoch 78\n",
      " train mean loss=0.020118912206962703\n",
      "epoch 79\n",
      " train mean loss=0.02260334549471736\n",
      "epoch 80\n",
      " train mean loss=0.02106274975463748\n",
      "epoch 81\n",
      " train mean loss=0.020519460514187814\n",
      "epoch 82\n",
      " train mean loss=0.020018420862033964\n",
      "epoch 83\n",
      " train mean loss=0.021411063522100447\n",
      "epoch 84\n",
      " train mean loss=0.02098974699154496\n",
      "epoch 85\n",
      " train mean loss=0.020994331007823348\n",
      "epoch 86\n",
      " train mean loss=0.02128057005815208\n",
      "epoch 87\n",
      " train mean loss=0.021066975705325605\n",
      "epoch 88\n",
      " train mean loss=0.020756566543132068\n",
      "epoch 89\n",
      " train mean loss=0.0215391359757632\n",
      "epoch 90\n",
      " train mean loss=0.019863046417012812\n",
      "epoch 91\n",
      " train mean loss=0.0211249759234488\n",
      "epoch 92\n",
      " train mean loss=0.019890970196574927\n",
      "epoch 93\n",
      " train mean loss=0.020106547763571143\n",
      "epoch 94\n",
      " train mean loss=0.019909962406381966\n",
      "epoch 95\n",
      " train mean loss=0.020845220740884542\n",
      "epoch 96\n",
      " train mean loss=0.019604573771357537\n",
      "epoch 97\n",
      " train mean loss=0.019814895996823907\n",
      "epoch 98\n",
      " train mean loss=0.018905597943812608\n",
      "epoch 99\n",
      " train mean loss=0.01846299831755459\n",
      "epoch 100\n",
      " train mean loss=0.018943598587065937\n",
      "epoch 101\n",
      " train mean loss=0.019518371596932413\n",
      "epoch 102\n",
      " train mean loss=0.018948995769023896\n",
      "epoch 103\n",
      " train mean loss=0.019577351789921524\n",
      "epoch 104\n",
      " train mean loss=0.019131764592602848\n",
      "epoch 105\n",
      " train mean loss=0.019165352899581194\n",
      "epoch 106\n",
      " train mean loss=0.018812422277405857\n",
      "epoch 107\n",
      " train mean loss=0.020003156987950205\n",
      "epoch 108\n",
      " train mean loss=0.01890944826416671\n",
      "epoch 109\n",
      " train mean loss=0.018719902895390987\n",
      "epoch 110\n",
      " train mean loss=0.021181426383554934\n",
      "epoch 111\n",
      " train mean loss=0.017689950512722135\n",
      "epoch 112\n",
      " train mean loss=0.017997673274949194\n",
      "epoch 113\n",
      " train mean loss=0.018834021436050535\n",
      "epoch 114\n",
      " train mean loss=0.018419416435062886\n",
      "epoch 115\n",
      " train mean loss=0.017505991952493787\n",
      "epoch 116\n",
      " train mean loss=0.0185326614882797\n",
      "epoch 117\n",
      " train mean loss=0.01850525655783713\n",
      "epoch 118\n",
      " train mean loss=0.017412729784846304\n",
      "epoch 119\n",
      " train mean loss=0.0178068935777992\n",
      "epoch 120\n",
      " train mean loss=0.01719922344200313\n",
      "epoch 121\n",
      " train mean loss=0.018420664072036744\n",
      "epoch 122\n",
      " train mean loss=0.017168889800086618\n",
      "epoch 123\n",
      " train mean loss=0.017244004001840948\n",
      "epoch 124\n",
      " train mean loss=0.016777396546676756\n",
      "epoch 125\n",
      " train mean loss=0.016673370441421868\n",
      "epoch 126\n",
      " train mean loss=0.01642746854573488\n",
      "epoch 127\n",
      " train mean loss=0.016487725311890244\n",
      "epoch 128\n",
      " train mean loss=0.016701319636777043\n",
      "epoch 129\n",
      " train mean loss=0.016785731771960855\n",
      "epoch 130\n",
      " train mean loss=0.016673281490802765\n",
      "epoch 131\n",
      " train mean loss=0.016527261789888142\n",
      "epoch 132\n",
      " train mean loss=0.016665280070155858\n",
      "epoch 133\n",
      " train mean loss=0.01811150128953159\n",
      "epoch 134\n",
      " train mean loss=0.016271191192790867\n",
      "epoch 135\n",
      " train mean loss=0.015866747815161942\n",
      "epoch 136\n",
      " train mean loss=0.016201197197660805\n",
      "epoch 137\n",
      " train mean loss=0.015984224164858462\n",
      "epoch 138\n",
      " train mean loss=0.016657037204131486\n",
      "epoch 139\n",
      " train mean loss=0.015946604423224927\n",
      "epoch 140\n",
      " train mean loss=0.015703393341973425\n",
      "epoch 141\n",
      " train mean loss=0.015351877082139254\n",
      "epoch 142\n",
      " train mean loss=0.015669314740225673\n",
      "epoch 143\n",
      " train mean loss=0.01623717773705721\n",
      "epoch 144\n",
      " train mean loss=0.015894903196021916\n",
      "epoch 145\n",
      " train mean loss=0.015486661391332746\n",
      "epoch 146\n",
      " train mean loss=0.0159456339571625\n",
      "epoch 147\n",
      " train mean loss=0.014610244473442436\n",
      "epoch 148\n",
      " train mean loss=0.014855845989659429\n",
      "epoch 149\n",
      " train mean loss=0.015284213684499264\n",
      "epoch 150\n",
      " train mean loss=0.014726566728204488\n",
      "epoch 151\n",
      " train mean loss=0.01462783637456596\n",
      "epoch 152\n",
      " train mean loss=0.014562920955941082\n",
      "epoch 153\n",
      " train mean loss=0.014518234319984913\n",
      "epoch 154\n",
      " train mean loss=0.014298703223466873\n",
      "epoch 155\n",
      " train mean loss=0.014201985793188215\n",
      "epoch 156\n",
      " train mean loss=0.015287715354934334\n",
      "epoch 157\n",
      " train mean loss=0.015341389635577797\n",
      "epoch 158\n",
      " train mean loss=0.014508411502465606\n",
      "epoch 159\n",
      " train mean loss=0.014324506316334008\n",
      "epoch 160\n",
      " train mean loss=0.014245459754019975\n",
      "epoch 161\n",
      " train mean loss=0.013770477194339038\n",
      "epoch 162\n",
      " train mean loss=0.014318389100953937\n",
      "epoch 163\n",
      " train mean loss=0.014333580117672683\n",
      "epoch 164\n",
      " train mean loss=0.01412187047302723\n",
      "epoch 165\n",
      " train mean loss=0.013991548921912908\n",
      "epoch 166\n",
      " train mean loss=0.014080116599798203\n",
      "epoch 167\n",
      " train mean loss=0.013984556663781405\n",
      "epoch 168\n",
      " train mean loss=0.014214104609563946\n",
      "epoch 169\n",
      " train mean loss=0.014205590039491654\n",
      "epoch 170\n",
      " train mean loss=0.013322414979338646\n",
      "epoch 171\n",
      " train mean loss=0.014022057261317968\n",
      "epoch 172\n",
      " train mean loss=0.013708832934498786\n",
      "epoch 173\n",
      " train mean loss=0.014047329286113382\n",
      "epoch 174\n",
      " train mean loss=0.013371347524225712\n",
      "epoch 175\n",
      " train mean loss=0.014113225555047394\n",
      "epoch 176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train mean loss=0.013974856669083238\n",
      "epoch 177\n",
      " train mean loss=0.013990797661244869\n",
      "epoch 178\n",
      " train mean loss=0.013640182306990028\n",
      "epoch 179\n",
      " train mean loss=0.013591573787853122\n",
      "epoch 180\n",
      " train mean loss=0.013756314376369119\n",
      "epoch 181\n",
      " train mean loss=0.013710111724212765\n",
      "epoch 182\n",
      " train mean loss=0.013602498136460781\n",
      "epoch 183\n",
      " train mean loss=0.013593616504222155\n",
      "epoch 184\n",
      " train mean loss=0.013338782647624612\n",
      "epoch 185\n",
      " train mean loss=0.013085242165252566\n",
      "epoch 186\n",
      " train mean loss=0.013495515091344714\n",
      "epoch 187\n",
      " train mean loss=0.013622666848823429\n",
      "epoch 188\n",
      " train mean loss=0.013906517000868916\n",
      "epoch 189\n",
      " train mean loss=0.013223455930128693\n",
      "epoch 190\n",
      " train mean loss=0.013112802212126552\n",
      "epoch 191\n",
      " train mean loss=0.013094524443149567\n",
      "epoch 192\n",
      " train mean loss=0.01272353702224791\n",
      "epoch 193\n",
      " train mean loss=0.013250394770875574\n",
      "epoch 194\n",
      " train mean loss=0.013559956038370728\n",
      "epoch 195\n",
      " train mean loss=0.012920572916045784\n",
      "epoch 196\n",
      " train mean loss=0.012752615390345454\n",
      "epoch 197\n",
      " train mean loss=0.012704404648393392\n",
      "epoch 198\n",
      " train mean loss=0.0125740775372833\n",
      "epoch 199\n",
      " train mean loss=0.012955956691876054\n",
      "epoch 200\n",
      " train mean loss=0.012299005882814526\n",
      "epoch 201\n",
      " train mean loss=0.012972670886665583\n",
      "epoch 202\n",
      " train mean loss=0.012823869278654455\n",
      "epoch 203\n",
      " train mean loss=0.01281559707596898\n",
      "epoch 204\n",
      " train mean loss=0.012550424709916115\n",
      "epoch 205\n",
      " train mean loss=0.012387375384569167\n",
      "epoch 206\n",
      " train mean loss=0.012543552806600928\n",
      "epoch 207\n",
      " train mean loss=0.012526162303984165\n",
      "epoch 208\n",
      " train mean loss=0.012247036285698414\n",
      "epoch 209\n",
      " train mean loss=0.01283213248476386\n",
      "epoch 210\n",
      " train mean loss=0.012566017233766616\n",
      "epoch 211\n",
      " train mean loss=0.012413266533985733\n",
      "epoch 212\n",
      " train mean loss=0.012468713810667395\n",
      "epoch 213\n",
      " train mean loss=0.01284531420096755\n",
      "epoch 214\n",
      " train mean loss=0.012205374529585242\n",
      "epoch 215\n",
      " train mean loss=0.01218825301155448\n",
      "epoch 216\n",
      " train mean loss=0.012328739259392024\n",
      "epoch 217\n",
      " train mean loss=0.012164107644930483\n",
      "epoch 218\n",
      " train mean loss=0.012284971307963133\n",
      "epoch 219\n",
      " train mean loss=0.011837925836443901\n",
      "epoch 220\n",
      " train mean loss=0.012075431607663632\n",
      "epoch 221\n",
      " train mean loss=0.012180601190775632\n",
      "epoch 222\n",
      " train mean loss=0.0119210164854303\n",
      "epoch 223\n",
      " train mean loss=0.011984817311167717\n",
      "epoch 224\n",
      " train mean loss=0.012167767910286785\n",
      "epoch 225\n",
      " train mean loss=0.0119272596295923\n",
      "epoch 226\n",
      " train mean loss=0.011885036705061794\n",
      "epoch 227\n",
      " train mean loss=0.011946953926235437\n",
      "epoch 228\n",
      " train mean loss=0.012355085806921124\n",
      "epoch 229\n",
      " train mean loss=0.012419843412935734\n",
      "epoch 230\n",
      " train mean loss=0.01202127581462264\n",
      "epoch 231\n",
      " train mean loss=0.012300102300941943\n",
      "epoch 232\n",
      " train mean loss=0.012164375530555844\n",
      "epoch 233\n",
      " train mean loss=0.011816240679472685\n",
      "epoch 234\n",
      " train mean loss=0.011568703236989678\n",
      "epoch 235\n",
      " train mean loss=0.011766632590442895\n",
      "epoch 236\n",
      " train mean loss=0.01188298460561782\n",
      "epoch 237\n",
      " train mean loss=0.011920861294493079\n",
      "epoch 238\n",
      " train mean loss=0.012105397619307042\n",
      "epoch 239\n",
      " train mean loss=0.011723158531822264\n",
      "epoch 240\n",
      " train mean loss=0.011650168430060148\n",
      "epoch 241\n",
      " train mean loss=0.012011255552060902\n",
      "epoch 242\n",
      " train mean loss=0.011745870332233607\n",
      "epoch 243\n",
      " train mean loss=0.011704619545489549\n",
      "epoch 244\n",
      " train mean loss=0.012207463262602686\n",
      "epoch 245\n",
      " train mean loss=0.01202036335133016\n",
      "epoch 246\n",
      " train mean loss=0.01236935395281762\n",
      "epoch 247\n",
      " train mean loss=0.011751599335111678\n",
      "epoch 248\n",
      " train mean loss=0.011575592756271362\n",
      "epoch 249\n",
      " train mean loss=0.0118677700124681\n",
      "epoch 250\n",
      " train mean loss=0.011826883004978299\n",
      "epoch 251\n",
      " train mean loss=0.011574020250700414\n",
      "epoch 252\n",
      " train mean loss=0.011672277245670557\n",
      "epoch 253\n",
      " train mean loss=0.012056696554645896\n",
      "epoch 254\n",
      " train mean loss=0.011523471120744944\n",
      "epoch 255\n",
      " train mean loss=0.01193590622395277\n",
      "epoch 256\n",
      " train mean loss=0.011751782214269042\n",
      "epoch 257\n",
      " train mean loss=0.011504966523498297\n",
      "epoch 258\n",
      " train mean loss=0.011842700969427824\n",
      "epoch 259\n",
      " train mean loss=0.011440650122240185\n",
      "epoch 260\n",
      " train mean loss=0.011443132911808788\n",
      "epoch 261\n",
      " train mean loss=0.011771318013779819\n",
      "epoch 262\n",
      " train mean loss=0.011491952007636428\n",
      "epoch 263\n",
      " train mean loss=0.011695962394587695\n",
      "epoch 264\n",
      " train mean loss=0.011514265807345509\n",
      "epoch 265\n",
      " train mean loss=0.011407654331997037\n",
      "epoch 266\n",
      " train mean loss=0.011577840745449066\n",
      "epoch 267\n",
      " train mean loss=0.01146067351102829\n",
      "epoch 268\n",
      " train mean loss=0.011536805490031839\n",
      "epoch 269\n",
      " train mean loss=0.011738599808886648\n",
      "epoch 270\n",
      " train mean loss=0.011851829141378403\n",
      "epoch 271\n",
      " train mean loss=0.01144526232033968\n",
      "epoch 272\n",
      " train mean loss=0.011628468399867415\n",
      "epoch 273\n",
      " train mean loss=0.011445911284536123\n",
      "epoch 274\n",
      " train mean loss=0.011271415455266833\n",
      "epoch 275\n",
      " train mean loss=0.011452207630500198\n",
      "epoch 276\n",
      " train mean loss=0.011198378121480345\n",
      "epoch 277\n",
      " train mean loss=0.011568158324807882\n",
      "epoch 278\n",
      " train mean loss=0.01154545231256634\n",
      "epoch 279\n",
      " train mean loss=0.011524571762420237\n",
      "epoch 280\n",
      " train mean loss=0.011417364259250462\n",
      "epoch 281\n",
      " train mean loss=0.01165324255824089\n",
      "epoch 282\n",
      " train mean loss=0.011857383167371154\n",
      "epoch 283\n",
      " train mean loss=0.011485175294801593\n",
      "epoch 284\n",
      " train mean loss=0.011971777803264558\n",
      "epoch 285\n",
      " train mean loss=0.011507448209449649\n",
      "epoch 286\n",
      " train mean loss=0.011658486537635326\n",
      "epoch 287\n",
      " train mean loss=0.011672863601706922\n",
      "epoch 288\n",
      " train mean loss=0.01126150990370661\n",
      "epoch 289\n",
      " train mean loss=0.01139292161911726\n",
      "epoch 290\n",
      " train mean loss=0.01146464480087161\n",
      "epoch 291\n",
      " train mean loss=0.011389100067317486\n",
      "epoch 292\n",
      " train mean loss=0.0112965321354568\n",
      "epoch 293\n",
      " train mean loss=0.01132774923928082\n",
      "epoch 294\n",
      " train mean loss=0.01171673770993948\n",
      "epoch 295\n",
      " train mean loss=0.011666989531368016\n",
      "epoch 296\n",
      " train mean loss=0.011333337454125285\n",
      "epoch 297\n",
      " train mean loss=0.011547923213802278\n",
      "epoch 298\n",
      " train mean loss=0.011301051983609796\n",
      "epoch 299\n",
      " train mean loss=0.011649666992016137\n",
      "epoch 300\n",
      " train mean loss=0.01125786058139056\n",
      "epoch 301\n",
      " train mean loss=0.011357257124036551\n",
      "epoch 302\n",
      " train mean loss=0.011416689301840961\n",
      "epoch 303\n",
      " train mean loss=0.011362807108089328\n",
      "epoch 304\n",
      " train mean loss=0.011620573960244655\n",
      "epoch 305\n",
      " train mean loss=0.011151446206495165\n",
      "epoch 306\n",
      " train mean loss=0.011322495136409998\n",
      "epoch 307\n",
      " train mean loss=0.011339154751040042\n",
      "epoch 308\n",
      " train mean loss=0.011489133164286614\n",
      "epoch 309\n",
      " train mean loss=0.011492231376469136\n",
      "epoch 310\n",
      " train mean loss=0.011301542674191296\n",
      "epoch 311\n",
      " train mean loss=0.011324426215142012\n",
      "epoch 312\n",
      " train mean loss=0.011349831093102693\n",
      "epoch 313\n",
      " train mean loss=0.011355635216459633\n",
      "epoch 314\n",
      " train mean loss=0.011446018973365425\n",
      "epoch 315\n",
      " train mean loss=0.011684814477339387\n",
      "epoch 316\n",
      " train mean loss=0.011516325436532497\n",
      "epoch 317\n",
      " train mean loss=0.011414342047646641\n",
      "epoch 318\n",
      " train mean loss=0.011260060155764222\n",
      "epoch 319\n",
      " train mean loss=0.011375464759767055\n",
      "epoch 320\n",
      " train mean loss=0.011311290487647056\n",
      "epoch 321\n",
      " train mean loss=0.011140858326107263\n",
      "epoch 322\n",
      " train mean loss=0.011136578358709813\n",
      "epoch 323\n",
      " train mean loss=0.011447418686002494\n",
      "epoch 324\n",
      " train mean loss=0.011118714679032565\n",
      "epoch 325\n",
      " train mean loss=0.011158482087776065\n",
      "epoch 326\n",
      " train mean loss=0.011349021028727292\n",
      "epoch 327\n",
      " train mean loss=0.010858773337677121\n",
      "epoch 328\n",
      " train mean loss=0.011422658287920058\n",
      "epoch 329\n",
      " train mean loss=0.011502954675816\n",
      "epoch 330\n",
      " train mean loss=0.011265050936490297\n",
      "epoch 331\n",
      " train mean loss=0.011531394384801387\n",
      "epoch 332\n",
      " train mean loss=0.011264954120852053\n",
      "epoch 333\n",
      " train mean loss=0.011070593753829598\n",
      "epoch 334\n",
      " train mean loss=0.011258128331974149\n",
      "epoch 335\n",
      " train mean loss=0.011398586044088006\n",
      "epoch 336\n",
      " train mean loss=0.011754224691540003\n",
      "epoch 337\n",
      " train mean loss=0.011133140344172716\n",
      "epoch 338\n",
      " train mean loss=0.011495644487440585\n",
      "epoch 339\n",
      " train mean loss=0.011240253811702133\n",
      "epoch 340\n",
      " train mean loss=0.011053175390698016\n",
      "epoch 341\n",
      " train mean loss=0.011056088311597705\n",
      "epoch 342\n",
      " train mean loss=0.011183499982580542\n",
      "epoch 343\n",
      " train mean loss=0.011792182959616183\n",
      "epoch 344\n",
      " train mean loss=0.011298020360991359\n",
      "epoch 345\n",
      " train mean loss=0.011104629104956985\n",
      "epoch 346\n",
      " train mean loss=0.011220625024288894\n",
      "epoch 347\n",
      " train mean loss=0.011335154552944005\n",
      "epoch 348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train mean loss=0.011036755116656423\n",
      "epoch 349\n",
      " train mean loss=0.011573868319392205\n",
      "epoch 350\n",
      " train mean loss=0.011235646242275833\n",
      "epoch 351\n",
      " train mean loss=0.01140313538722694\n",
      "epoch 352\n",
      " train mean loss=0.011106185894459487\n",
      "epoch 353\n",
      " train mean loss=0.011200605556368828\n",
      "epoch 354\n",
      " train mean loss=0.011456352923996747\n",
      "epoch 355\n",
      " train mean loss=0.010981490011326969\n",
      "epoch 356\n",
      " train mean loss=0.01088329624850303\n",
      "epoch 357\n",
      " train mean loss=0.01120853317901492\n",
      "epoch 358\n",
      " train mean loss=0.011306535415351392\n",
      "epoch 359\n",
      " train mean loss=0.010982389333657921\n",
      "epoch 360\n",
      " train mean loss=0.011503492761403323\n",
      "epoch 361\n",
      " train mean loss=0.011077142329886555\n",
      "epoch 362\n",
      " train mean loss=0.011247233618050814\n",
      "epoch 363\n",
      " train mean loss=0.011040353751741349\n",
      "epoch 364\n",
      " train mean loss=0.011317300288937985\n",
      "epoch 365\n",
      " train mean loss=0.011023550690151752\n",
      "epoch 366\n",
      " train mean loss=0.010952922198921442\n",
      "epoch 367\n",
      " train mean loss=0.01139405719935894\n",
      "epoch 368\n",
      " train mean loss=0.011225740583613515\n",
      "epoch 369\n",
      " train mean loss=0.011091709211468697\n",
      "epoch 370\n",
      " train mean loss=0.011153196934610605\n",
      "epoch 371\n",
      " train mean loss=0.011169131798669695\n",
      "epoch 372\n",
      " train mean loss=0.011608806001022457\n",
      "epoch 373\n",
      " train mean loss=0.011571360705420374\n",
      "epoch 374\n",
      " train mean loss=0.011591029632836581\n",
      "epoch 375\n",
      " train mean loss=0.011297184834256768\n",
      "epoch 376\n",
      " train mean loss=0.011432648748159408\n",
      "epoch 377\n",
      " train mean loss=0.011298390813171863\n",
      "epoch 378\n",
      " train mean loss=0.011051814276725054\n",
      "epoch 379\n",
      " train mean loss=0.010702766189351677\n",
      "epoch 380\n",
      " train mean loss=0.010981437135487795\n",
      "epoch 381\n",
      " train mean loss=0.01073882196098566\n",
      "epoch 382\n",
      " train mean loss=0.01122177272103727\n",
      "epoch 383\n",
      " train mean loss=0.011021807789802551\n",
      "epoch 384\n",
      " train mean loss=0.011199837252497673\n",
      "epoch 385\n",
      " train mean loss=0.011279946491122246\n",
      "epoch 386\n",
      " train mean loss=0.011091213980689645\n",
      "epoch 387\n",
      " train mean loss=0.01116542894858867\n",
      "epoch 388\n",
      " train mean loss=0.011494324025698006\n",
      "epoch 389\n",
      " train mean loss=0.011346371858380735\n",
      "epoch 390\n",
      " train mean loss=0.011052542543038725\n",
      "epoch 391\n",
      " train mean loss=0.011608700435608625\n",
      "epoch 392\n",
      " train mean loss=0.011249080984853208\n",
      "epoch 393\n",
      " train mean loss=0.011206602528691292\n",
      "epoch 394\n",
      " train mean loss=0.010948245953768492\n",
      "epoch 395\n",
      " train mean loss=0.011268601943738759\n",
      "epoch 396\n",
      " train mean loss=0.011388990702107549\n",
      "epoch 397\n",
      " train mean loss=0.011115907924249768\n",
      "epoch 398\n",
      " train mean loss=0.011210091095417738\n",
      "epoch 399\n",
      " train mean loss=0.011419320893473923\n",
      "epoch 400\n",
      " train mean loss=0.011215636865235865\n",
      "epoch 401\n",
      " train mean loss=0.011285386667586862\n",
      "epoch 402\n",
      " train mean loss=0.011074667763896286\n",
      "epoch 403\n",
      " train mean loss=0.010985850282013416\n",
      "epoch 404\n",
      " train mean loss=0.010894993771798908\n",
      "epoch 405\n",
      " train mean loss=0.011148141343146562\n",
      "epoch 406\n",
      " train mean loss=0.01149388057179749\n",
      "epoch 407\n",
      " train mean loss=0.011083989106118679\n",
      "epoch 408\n",
      " train mean loss=0.011135949427261949\n",
      "epoch 409\n",
      " train mean loss=0.011068077930249274\n",
      "epoch 410\n",
      " train mean loss=0.011226000804454088\n",
      "epoch 411\n",
      " train mean loss=0.01096052625682205\n",
      "epoch 412\n",
      " train mean loss=0.01147947957739234\n",
      "epoch 413\n",
      " train mean loss=0.011032559806481004\n",
      "epoch 414\n",
      " train mean loss=0.01146080993115902\n",
      "epoch 415\n",
      " train mean loss=0.01150469260290265\n",
      "epoch 416\n",
      " train mean loss=0.010869706175290048\n",
      "epoch 417\n",
      " train mean loss=0.01115757206454873\n",
      "epoch 418\n",
      " train mean loss=0.010872249393723905\n",
      "epoch 419\n",
      " train mean loss=0.01093260568100959\n",
      "epoch 420\n",
      " train mean loss=0.010832890351302922\n",
      "epoch 421\n",
      " train mean loss=0.011351811517961323\n",
      "epoch 422\n",
      " train mean loss=0.011072583510540426\n",
      "epoch 423\n",
      " train mean loss=0.01136867851484567\n",
      "epoch 424\n",
      " train mean loss=0.01089585550595075\n",
      "epoch 425\n",
      " train mean loss=0.011121685644611716\n",
      "epoch 426\n",
      " train mean loss=0.011144969379529357\n",
      "epoch 427\n",
      " train mean loss=0.01094511678442359\n",
      "epoch 428\n",
      " train mean loss=0.011096085282042622\n",
      "epoch 429\n",
      " train mean loss=0.011257360083982348\n",
      "epoch 430\n",
      " train mean loss=0.011003263080492615\n",
      "epoch 431\n",
      " train mean loss=0.011050217314623295\n",
      "epoch 432\n",
      " train mean loss=0.011538382526487112\n",
      "epoch 433\n",
      " train mean loss=0.01129558218177408\n",
      "epoch 434\n",
      " train mean loss=0.011105163926258684\n",
      "epoch 435\n",
      " train mean loss=0.011301711970008909\n",
      "epoch 436\n",
      " train mean loss=0.011249511237256229\n",
      "epoch 437\n",
      " train mean loss=0.011095999572426081\n",
      "epoch 438\n",
      " train mean loss=0.011527661187574267\n",
      "epoch 439\n",
      " train mean loss=0.011044575073756278\n",
      "epoch 440\n",
      " train mean loss=0.011246693818829954\n",
      "epoch 441\n",
      " train mean loss=0.010859809764660895\n",
      "epoch 442\n",
      " train mean loss=0.01101187418680638\n",
      "epoch 443\n",
      " train mean loss=0.010803504199720919\n",
      "epoch 444\n",
      " train mean loss=0.011034574140794576\n",
      "epoch 445\n",
      " train mean loss=0.011118324380367995\n",
      "epoch 446\n",
      " train mean loss=0.01101047169417143\n",
      "epoch 447\n",
      " train mean loss=0.010950886607170106\n",
      "epoch 448\n",
      " train mean loss=0.011104630734771491\n",
      "epoch 449\n",
      " train mean loss=0.011206709342077374\n",
      "epoch 450\n",
      " train mean loss=0.011171978530474008\n",
      "epoch 451\n",
      " train mean loss=0.011044729468412697\n",
      "epoch 452\n",
      " train mean loss=0.011167278997600078\n",
      "epoch 453\n",
      " train mean loss=0.010777429100126029\n",
      "epoch 454\n",
      " train mean loss=0.010948909097351134\n",
      "epoch 455\n",
      " train mean loss=0.010881335251033306\n",
      "epoch 456\n",
      " train mean loss=0.011230912879109383\n",
      "epoch 457\n",
      " train mean loss=0.011103593669831753\n",
      "epoch 458\n",
      " train mean loss=0.011112667466513813\n",
      "epoch 459\n",
      " train mean loss=0.010883147404529154\n",
      "epoch 460\n",
      " train mean loss=0.010642614816315472\n",
      "epoch 461\n",
      " train mean loss=0.011209572004154324\n",
      "epoch 462\n",
      " train mean loss=0.010912429974414408\n",
      "epoch 463\n",
      " train mean loss=0.011278328034095467\n",
      "epoch 464\n",
      " train mean loss=0.011147473761811853\n",
      "epoch 465\n",
      " train mean loss=0.01081355128902942\n",
      "epoch 466\n",
      " train mean loss=0.011159244235605001\n",
      "epoch 467\n",
      " train mean loss=0.010898496597073972\n",
      "epoch 468\n",
      " train mean loss=0.010834658867679536\n",
      "epoch 469\n",
      " train mean loss=0.010738876550458371\n",
      "epoch 470\n",
      " train mean loss=0.011264142813161016\n",
      "epoch 471\n",
      " train mean loss=0.011225636596791447\n",
      "epoch 472\n",
      " train mean loss=0.01089360578916967\n",
      "epoch 473\n",
      " train mean loss=0.011072001978754997\n",
      "epoch 474\n",
      " train mean loss=0.010772825689055026\n",
      "epoch 475\n",
      " train mean loss=0.010926743987947702\n",
      "epoch 476\n",
      " train mean loss=0.011076447442173959\n",
      "epoch 477\n",
      " train mean loss=0.010878832754679024\n",
      "epoch 478\n",
      " train mean loss=0.011061509307473898\n",
      "epoch 479\n",
      " train mean loss=0.010919879055581987\n",
      "epoch 480\n",
      " train mean loss=0.010902271643280982\n",
      "epoch 481\n",
      " train mean loss=0.011305841295979916\n",
      "epoch 482\n",
      " train mean loss=0.011007343125529588\n",
      "epoch 483\n",
      " train mean loss=0.011156603619456291\n",
      "epoch 484\n",
      " train mean loss=0.010847411239519716\n",
      "epoch 485\n",
      " train mean loss=0.010687607061117887\n",
      "epoch 486\n",
      " train mean loss=0.011189629090949893\n",
      "epoch 487\n",
      " train mean loss=0.011021861429326237\n",
      "epoch 488\n",
      " train mean loss=0.011079662176780403\n",
      "epoch 489\n",
      " train mean loss=0.011112789842300116\n",
      "epoch 490\n",
      " train mean loss=0.010947237601503729\n",
      "epoch 491\n",
      " train mean loss=0.011071335510350763\n",
      "epoch 492\n",
      " train mean loss=0.010955837052315474\n",
      "epoch 493\n",
      " train mean loss=0.011128118541091681\n",
      "epoch 494\n",
      " train mean loss=0.010968844313174487\n",
      "epoch 495\n",
      " train mean loss=0.010997977047227324\n",
      "epoch 496\n",
      " train mean loss=0.01098538284189999\n",
      "epoch 497\n",
      " train mean loss=0.011438275435939432\n",
      "epoch 498\n",
      " train mean loss=0.010962806241586805\n",
      "epoch 499\n",
      " train mean loss=0.010786370472051204\n",
      "epoch 500\n",
      " train mean loss=0.01092055241111666\n"
     ]
    }
   ],
   "source": [
    "T=[]\n",
    "for epoch in range(1,n_epoch+1):\n",
    "    print('epoch',epoch)\n",
    "    #t=clock()\n",
    "    perm=np.random.permutation(N)\n",
    "    sum_loss=0\n",
    "    for i in range(0,N,batchsize):\n",
    "        x_batch=x_train[perm[i:i+batchsize]]\n",
    "        y_batch=y_train[perm[i:i+batchsize]]\n",
    "        fi_batch=np.ones((batchsize,1)).astype(np.float32)\n",
    "        \n",
    "        optimizer.zero_grads()\n",
    "        loss=model(x_batch,y_batch,fi_batch)\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        sum_loss+=float(cuda.to_cpu(loss.data))*len(y_batch)\n",
    "    print(' train mean loss={}'.format(sum_loss/N))\n",
    "    T.append(sum_loss/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean loss=0.005591464776080102\n"
     ]
    }
   ],
   "source": [
    "sum_loss=0\n",
    "for i in range(0,N_test,batchsize):\n",
    "    x_batch=x_test[i:i+batchsize]\n",
    "    y_batch=y_test[i:i+batchsize]\n",
    "    fi_batch=np.ones((batchsize,1)).astype(np.float32)\n",
    "    \n",
    "    loss=model(x_batch,y_batch,fi_batch)\n",
    "    sum_loss+=float(cuda.to_cpu(loss.data))*len(y_batch)\n",
    "print('test mean loss={}'.format(sum_loss/N_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc93a5fe518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, n_epoch)\n",
    "y = T\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
